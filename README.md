# research

# 다음과 같은 논문을 참조하였습니다


#[1]	S. Hochreiter and J. Urgen Schmidhuber, “Long Shortterm Memory,” Neural Comput., vol. 9, no. 8, p. 17351780, 1997, [Online]. Available: http://www7.informatik.tu-muenchen.de/~hochreit%0Ahttp://www.idsia.ch/~juergen.

#[2]	G. Lai, W. C. Chang, Y. Yang, and H. Liu, “Modeling long- and short-term temporal patterns with deep neural networks,” 41st Int. ACM SIGIR Conf. Res. Dev. Inf. Retrieval, SIGIR 2018, no. July, pp. 95–104, 2018, doi: 10.1145/3209978.3210006.

#[3]	A. Vaswani et al., “Attention is all you need,” Adv. Neural Inf. Process. Syst., vol. 2017-Decem, no. Nips, pp. 5999–6009, 2017.

#[4]	S. Li et al., “Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,” arXiv, no. NeurIPS, 2019.

#[5]	H. Song, D. Rajan, J. J. Thiagarajan, and A. Spanias, “Attend and diagnose: Clinical time series analysis using attention models,” 32nd AAAI Conf. Artif. Intell. AAAI 2018, pp. 4091–4098, 2018.

#[6]	N. Wu, B. Green, X. Ben, and S. O’Banion, “Deep transformer models for time series forecasting: The influenza prevalence case,” arXiv, 2020.

#[7]	Z. Hu, Y. Zhao, and M. Khushi, “A Survey of Forex and Stock Price Prediction Using Deep Learning,” Appl. Syst. Innov., vol. 4, no. 1, p. 9, 2021, doi: 10.3390/asi4010009.

#[8]	T. W. Kim, “Portfolio Optimization with 2D Relative-Attentional Gated Transformer,” 2020.

#[9]	A. Graves, “Generating Sequences With Recurrent Neural Networks,” pp. 1–43, 2013, [Online]. Available: http://arxiv.org/abs/1308.0850.

#[10]	S. M. Kazemi et al., “Time2Vec: Learning a vector representation of time,” arXiv, 2019.

#[11]	H. Zhou et al., “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,” 2020, [Online]. Available: http://arxiv.org/abs/2012.07436.

#[12]	C. Kyunghyun, B. van Merriënboer, B. Dzmitry, and Y. Bengio, “On the Properties of Neural Machine Translation: Endoer-Decoder Approaches,” Proc. SSST-8, Eighth Work. Syntax. Semant. Struct. Stat. Transl., vol. 1, pp. 103–111, 2014.

#[13]	I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” Adv. Neural Inf. Process. Syst., vol. 4, no. January, pp. 3104–3112, 2014.

#[14]	R. Yu, S. Zheng, A. Anandkumar, and Y. Yue, “Long-term forecasting using higher-order tensor RNNs,” arXiv, vol. 1, pp. 1–24, 2017.

#[15]	Y. Liu, “attention mechanism including the dual-stage two-phase ( DSTP ) model and the influence mechanism of target information and non-target information , we propose DSTP-based RNN,” 2018.

#[16]	Y. Qin, D. Song, H. Cheng, W. Cheng, G. Jiang, and G. W. Cottrell, “A dual-stage attention-based recurrent neural network for time series prediction,” IJCAI Int. Jt. Conf. Artif. Intell., vol. 0, pp. 2627–2633, 2017, doi: 10.24963/ijcai.2017/366.


